<!--
第一周作业
1.Attention is all you need 论文解析(请加入自己的见解)
2.Computing Machinery and Intelligence论文分析（选做）
-->
# 第一周作业
1. Attention is all you need 论文解析(请加入自己的见解)
2. Computing Machinery and Intelligence论文分析（选做）

本周作业(仅以上内容，其他内容移至下周)手写后拍照发至助教邮箱

<!--
1012997105@qq.com
-->
[1012997105@qq.com](mailto:1012997105@qq.com)

标题注明：人工智能基础作业—姓名—班级—学号

---
## 1
### Summary
Before then, the sequence transduction models were mostly based on RNNs/CNNs, which is complex and computationally expensive. While attention mechanisms were already integrated into some of these models, the Transformer, the model they built, is solely based on attention mechanisms, offering an elegant and efficient architecture. It achieves SOTA results on WMT machine translation tasks with substantially reduced training times. The Transformer excels at modeling the dependencies regardless of their distance in the sequence. They also introduce Multi-Head Attention, which allows the model to jointly attend to information from different representation subspaces at different positions, thereby further improving performance.

### Model Details
#### Encoder and Decoder Stacks
The encoder is composed of a stack of 6 identical layers. Each layer has 2 sub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, position-wise fully connected feed-forward network. Each of these two sub-layers employs a residual connection, followed by layer normalization.

The decoder is similar to the encoder, except that each layer inserts a third sub-layer in each layer, performing multi-head attention over the output of the encoder stack. Additionally, the self-attention sub-layer is modified to prevent positions from attending to subsequent positions.

#### Attention
It can be described as mapping a query and a set of key-value pairs to an output, where they are all vectors. The output is a weighted sum of the values, where the weights are computed by a compatibility function of the query with the corresponding key.

##### Scaled Dot-Product
$$
\text{Attention}(\mathbf{Q}, \mathbf{K}, \mathbf{V}) = \text{softmax}(\frac{\mathbf{Q} \mathbf{K}^T}{\sqrt{d_k}}) \mathbf{V}
$$

Dot-product attention is preferred over additive attention in the 2 most commonly used attention functions because it's much faster and more space-efficient in practice, since it can be implemented using highly optimized matrix multiplication code.

The dot product is scaled by $\frac{1}{\sqrt{d_k}}$ because they suspect that for large values of the dimension of keys, the dot product grow large in magnitude, pushing the softmax function into regions where it has extremely small gradients.

##### Multi-Head Attention
$$\begin{align}
\mathbf{head}_i & = \text{Attention}(\mathbf{Q} \mathbf{W}_i^\mathbf{Q}, \mathbf{K} \mathbf{W}_i^\mathbf{K}, \mathbf{V} \mathbf{W}_i^\mathbf{V}) \\

\text{MultiHead}(\mathbf{Q}, \mathbf{K}, \mathbf{V}) & = \text{Concat}(\mathbf{head}_1, \cdots, \mathbf{head}_h) \mathbf{W}^O \\
\end{align}$$

Instead of performing a single attention function, it's found beneficial to linearly project them h times with different, learned linear projections and then concat and project back. On each of these projected versions, they can perform the attention function in parallel.

This allows the model to jointly attend to information from different representation subspaces at different positions. With a single attention head, averaging inhibits this.

#### Positional Encoding
$$\begin{align}
\text{PE}(pos, 2 i) & = \sin(\frac{pos}{10^\frac{8 i}{d_{model}}}) \\
\text{PE}(pos, 2 i + 1) & = \cos(\frac{pos}{10^\frac{8 i}{d_{model}}}) \\
\end{align}$$

Since their model contains no information about the order of relative or absolute position of the tokens in the sequence, positional encodings are added to the input embeddings at the bottoms of both encoder and decoder stacks.

There are many choices of positional encodings, fixed and learned. They use fixed sine and cosine functions of different frequencies because it was hypothesized that this would allow the model to easily learn to attend by relative positions. They also experimented with using learned positional embeddings, and found that these two yielded nearly identical results. Consequently, the former were chosen as it may allow the model to extrapolate to sequence longer than the ones encountered during training.

### Results
To evaluate the importance of different components of the Transformer, they varied the base model in different ways, measuring the change in performance on English-to-German translation on the development set, newstest2013.

They found that while single-head attention is 0.9 BLEU worse than the best setting, quality also drops off with too many heads. Reducing the attention key size also hurts model quality, suggesting that determining compatibility is not easy and that a more sophisticated compatibility function than dot product may be beneficial. They also observed that bigger models are better, and dropout is very helpful in avoiding over-fitting.

### Personal comments
I think that it's the computational efficiency and parallelizable nature that makes the Transformer beat RNNs/CNNs, thereby making it possible to understand long contexts. It's surprising to see that the simpler Transformer is better than other models which also incorporate attention mechanisms, which are the heart of Transformer. It's also interesting to see that this model has been a building block of the LLMs, somehow exhibit general intelligence. Is it the result of being efficient and scaling it both in terms of parameters and big data?
